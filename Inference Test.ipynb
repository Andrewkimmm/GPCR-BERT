{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrewkim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertModel, RobertaTokenizer, BertTokenizer, RobertaTokenizerFast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_topk_accuracy(preds,targets,topk,ignore_idx,num_classes):\n",
    "\n",
    "    topk_indices = torch.topk(preds, topk, dim=1)[1]\n",
    "    topk_indices = topk_indices.cpu()\n",
    "    targets = targets.cpu()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    for i in range(targets.shape[0]):\n",
    "        if targets[i] == ignore_idx:\n",
    "            continue\n",
    "        if targets[i] in topk_indices[i]:\n",
    "            total_correct += 1\n",
    "        total_count += 1\n",
    "    if total_count == 0:\n",
    "        print(\"debug target\")\n",
    "        print(targets)\n",
    "    accuracy = total_correct / total_count\n",
    "    return accuracy\n",
    "\n",
    "def torch_metrics_accuracy(preds,targets,ignore_idx,num_classes):\n",
    "    preds = preds.cpu()\n",
    "    targets = targets.cpu()\n",
    "    accuracy_obj = MulticlassAccuracy(num_classes,\n",
    "                                      average='micro',\n",
    "                                      ignore_index=ignore_idx)\n",
    "    accuracy = accuracy_obj(preds, targets)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.25\n",
    "batch_size = 4\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "distance_mask_token = 2\n",
    "no_mask_token = 1\n",
    "max_len = 371\n",
    "bert_max_len = 373\n",
    "layer_no_to_view = 29\n",
    "head_no_to_view = 0\n",
    "\n",
    "#data_path = '/home/andrewkim/Desktop/GPCRBert/data/final_edry_class.npy'\n",
    "data_path = '/home/andrewkim/Desktop/GPCRBert/data/final_cwxp_class.npy'\n",
    "#data_path = '/home/andrewkim/Desktop/GPCRBert/data/final_npxxy_class.npy'\n",
    "#parameter_path = '/home/andrewkim/Desktop/GPCRBert/parameter/proteins_EDRY.pt'\n",
    "parameter_path = '/home/andrewkim/Desktop/GPCRBert/parameter/proteins_CWXP.pt'\n",
    "#parameter_path = '/home/andrewkim/Desktop/GPCRBert/parameter/proteins_NPXXY.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A A D E V W V V G M G I V M S L I V L A I V F G N V L V I T A I A K F E R L Q T V T N Y F I T S L A C A D L V M G L A V V P F G A A C I L T K T W T F G N F W C E F W T S I D V L C V T A S I E T L C V I A V D R Y F A I T S P F K Y Q S L L T K N K A R V I I L M V W I V S G L T S F L P I Q M H W Y R A T H Q E A I N C Y A E E T C C D F F T N Q A Y A I A S S I V S F Y V P L V I M V F V Y S R V F Q E A K R Q L Q X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X K F A L K E H K A L K T L G I I M G T F T L C W J P F F I V N I V H V I Q D N L I R K E V Y I L L N W I G Y V N S G F N P L I Y C R S P D F R I A F Q E L L C L\n",
      "J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J L J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J J\n"
     ]
    }
   ],
   "source": [
    "### EXPERIMENT\n",
    "\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "pdb = data[0][0]\n",
    "seq = list(data[0][1]) # inputfull\n",
    "seq_join = ''.join(seq) # input_full_str\n",
    "pad_start = len(seq_join) + 1 # +1 for the [CLS] token\n",
    "\n",
    "# findind start and end of req_pre_string\n",
    "motif = data[0][2] # req_pre\n",
    "motif_join = ''.join(motif) # requ_pre_str\n",
    "start_idx = seq_join.find(motif_join)\n",
    "end_idx = start_idx + 1\n",
    "\n",
    "seq_list = list(seq_join)\n",
    "seq_list[start_idx + distance_mask_token : start_idx + distance_mask_token + no_mask_token] = 'J' * no_mask_token\n",
    "label_list = list(seq_join)\n",
    "label_list[:start_idx+distance_mask_token] = 'J'*len(label_list[:start_idx+distance_mask_token])\n",
    "label_list[start_idx+distance_mask_token+no_mask_token:] = 'J'*len(label_list[start_idx+distance_mask_token+no_mask_token:])\n",
    "\n",
    "seq_list_spaced = ' '.join(seq_list)\n",
    "label_list_spaced = ' '.join(label_list)\n",
    "print(seq_list_spaced)\n",
    "print(label_list_spaced)\n",
    "seq_list_spaced = seq_list_spaced.replace('J', '[MASK]')\n",
    "label_list_spaced = label_list_spaced.replace('J', '[MASK]')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')\n",
    "seq_tokenized = tokenizer(seq_list_spaced, return_tensors='pt', padding='max_length', max_length=bert_max_len)\n",
    "label_tokenized = tokenizer(label_list_spaced, return_tensors='pt', padding='max_length', max_length=bert_max_len)\n",
    "\n",
    "label_tokenized['input_ids'][label_tokenized['input_ids'] == 4] = 0\n",
    "label_tokenized['input_ids'][label_tokenized['input_ids'] == 3] = 0\n",
    "label_tokenized['input_ids'][label_tokenized['input_ids'] == 2] = 0\n",
    "label_tokenized['input_ids'][label_tokenized['input_ids'] == 1] = 0\n",
    "\n",
    "# convert attention of mask to 0 in input_tokenized which is 4\n",
    "seq_tokenized['attention_mask'][seq_tokenized['input_ids'] == 4] = 0\n",
    "\n",
    "seq_vocab = tokenizer.convert_ids_to_tokens(seq_tokenized['input_ids'][0])\n",
    "label_vocab = tokenizer.convert_ids_to_tokens(label_tokenized['input_ids'][0])\n",
    "\n",
    "#print(seq_tokenized, label_tokenized, start_idx+1, end_idx+1, pdb)\n",
    "\n",
    "# bert_vocab = dataset.tokenizer.vocab\n",
    "# inverse_vocab = {v: k for k, v in bert_vocab.items()}\n",
    "\n",
    "\n",
    "# for b_no, (input_token, label_token) in enumerate(zip(seq_tokenized['input_ids'], \n",
    "#                                                               label_tokenized['input_ids'])):\n",
    "#     #print(b_no, input_token, label_token)\n",
    "\n",
    "#     input_str_list = [inverse_vocab[int(x)] for x in input_token.tolist()]\n",
    "#     #print(input_str_list)\n",
    "#     input_str = \"\".join(input_str_list)\n",
    "#     #print(input_str)\n",
    "#     label_str_list = [inverse_vocab[int(x)] for x in label_token.tolist()]\n",
    "#     label_str = \"\".join(label_str_list)\n",
    "#     #print(input_str, label_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionPredictionFullDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,df) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')\n",
    "        print(f\"vocabulary {self.tokenizer.vocab}\")\n",
    "\n",
    "        # # finding an alphabet that is not in the vocabulary\n",
    "        # capital_letters = [chr(i) for i in range(ord('A'), ord('Z')+1)]\n",
    "        # for letter in capital_letters:\n",
    "        #     if letter not in self.tokenizer.vocab:\n",
    "        #         print(f\"letter {letter} not in vocabulary\")\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.bert_max_len = bert_max_len\n",
    "        self.my_mask_token = 'J'\n",
    "        self.bert_mask_token = '[MASK]'\n",
    "        self.distance_mask_token = distance_mask_token\n",
    "        self.no_mask_token = no_mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = list(self.df[idx][1]) # inputfull\n",
    "        seq_join = ''.join(seq) # input_full_str\n",
    "        pad_start = len(seq_join) + 1 # +1 for the [CLS] token\n",
    "\n",
    "        # findind start and end of req_pre_string\n",
    "        motif = self.df[idx][2] # req_pre\n",
    "        motif_join = ''.join(motif) # requ_pre_str\n",
    "        start_idx = seq_join.find(motif_join)\n",
    "        end_idx = start_idx + self.distance_mask_token + self.no_mask_token\n",
    "\n",
    "        # replacing the two positions with mask token\n",
    "        seq_list = list(seq_join)\n",
    "        seq_list[start_idx + self.distance_mask_token : start_idx + self.distance_mask_token + self.no_mask_token] = self.my_mask_token * self.no_mask_token\n",
    "        label_list = list(seq_join)\n",
    "        label_list[:start_idx+self.distance_mask_token] = self.my_mask_token*len(label_list[:start_idx+self.distance_mask_token])\n",
    "        label_list[start_idx+self.distance_mask_token+self.no_mask_token:] = self.my_mask_token*len(label_list[start_idx+self.distance_mask_token+self.no_mask_token:])\n",
    "\n",
    "        seq_list_spaced = ' '.join(seq_list)\n",
    "        label_list_spaced = ' '.join(label_list)\n",
    "\n",
    "        seq_list_spaced = seq_list_spaced.replace(self.my_mask_token, self.bert_mask_token)\n",
    "        seq_tokenized = self.tokenizer(seq_list_spaced, return_tensors='pt', padding='max_length', max_length=self.bert_max_len)\n",
    "\n",
    "        label_list_spaced = label_list_spaced.replace(self.my_mask_token, self.bert_mask_token)\n",
    "        label_tokenized = self.tokenizer(label_list_spaced, return_tensors='pt', padding='max_length', max_length=self.bert_max_len)\n",
    "\n",
    "        # convert label_full_tokenized to 4 to 0 (??)\n",
    "        label_tokenized['input_ids'][label_tokenized['input_ids'] == 4] = 0\n",
    "        label_tokenized['input_ids'][label_tokenized['input_ids'] == 3] = 0\n",
    "        label_tokenized['input_ids'][label_tokenized['input_ids'] == 2] = 0\n",
    "        label_tokenized['input_ids'][label_tokenized['input_ids'] == 1] = 0\n",
    "\n",
    "        # convert attention of mask to 0 in input_tokenized which is 4\n",
    "        seq_tokenized['attention_mask'][seq_tokenized['input_ids'] == 4] = 0\n",
    "        # print(label_tokenized['input_ids'])\n",
    "        seq_vocab = self.tokenizer.convert_ids_to_tokens(seq_tokenized['input_ids'][0])\n",
    "        label_vocab = self.tokenizer.convert_ids_to_tokens(label_tokenized['input_ids'][0])\n",
    "\n",
    "        pdb = self.df[idx][0]\n",
    "        seq_len = len(self.df[idx][1])\n",
    "\n",
    "        return seq_tokenized, label_tokenized, start_idx+self.distance_mask_token+1, end_idx+1, pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTwoPositionsBert(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ClassificationTwoPositionsBert, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained('Rostlab/prot_bert', output_attentions=True)\n",
    "\n",
    "        for key, value in self.encoder.encoder.named_parameters():\n",
    "            layer_num = int(key.split('.')[1])\n",
    "            if layer_num < 23:\n",
    "                value.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(1024,256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(256,30))\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        # input_tokens: {input_ids, attention_mask, token_type_ids}\n",
    "        encoded_features = self.encoder(**input_tokens)['last_hidden_state']\n",
    "        # N*embedding_length*30\n",
    "        logits = self.fc(encoded_features)\n",
    "        logits = torch.permute(logits, (0, 2, 1))\n",
    "        return logits # N*30*embedding_length\n",
    "\n",
    "    def forward_test(self, input_tokens):\n",
    "        # input_tokens: {input_ids, attention_mask, token_type_ids}\n",
    "        encoded = self.encoder(**input_tokens)\n",
    "        encoded_features = encoded['last_hidden_state'] # N*max_len*hidden_dims\n",
    "        attentions = encoded['attentions']\n",
    "        logits = self.fc(encoded_features)\n",
    "        logits = torch.permute(logits, (0, 2, 1))\n",
    "        return logits, attentions # N*classes*max_len, N*num_heads*max_len*max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_two_position_prediction(model,data_loader,loss_fn,device):\n",
    "\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_gts = torch.zeros((0), dtype=torch.long).to(device)\n",
    "    total_preds = torch.zeros((0), dtype=torch.long).to(device)\n",
    "    total_logits = torch.zeros((0, 30),dtype=torch.float).to(device)\n",
    "    loop = tqdm(data_loader,leave=True,total=len(data_loader),colour='green')\n",
    "\n",
    "    for idx, batch in enumerate(loop):\n",
    "        input_tokens,label_tokens, start_idx, end_idx, pdb = batch\n",
    "\n",
    "        input_tokens['input_ids'] = input_tokens['input_ids'].to(device).squeeze(1)\n",
    "        input_tokens['token_type_ids'] = input_tokens['token_type_ids'].to(device).squeeze(1)\n",
    "        input_tokens['attention_mask'] = input_tokens['attention_mask'].to(device).squeeze(1)\n",
    "\n",
    "        label_tokens['input_ids'] = label_tokens['input_ids'].to(device).squeeze(1)\n",
    "        label_tokens['token_type_ids'] = label_tokens['token_type_ids'].to(device).squeeze(1)\n",
    "        label_tokens['attention_mask'] = label_tokens['attention_mask'].to(device).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_tokens)\n",
    "            iteration_loss = loss_fn(out, label_tokens['input_ids'])\n",
    "\n",
    "        total_loss += iteration_loss.item() * input_tokens['input_ids'].shape[0]\n",
    "        total_count += input_tokens['input_ids'].shape[0]\n",
    "        out = out.permute(0, 2, 1)\n",
    "\n",
    "        for b_idx in range(out.shape[0]):\n",
    "\n",
    "            #print(start_idx[b_idx], end_idx[b_idx])\n",
    "            for seq_idx in range(start_idx[b_idx], end_idx[b_idx]):\n",
    "                \n",
    "                total_logits = torch.cat((total_logits, out[b_idx, seq_idx, :].reshape(1, -1)), dim=0)\n",
    "                total_gts = torch.cat((total_gts, label_tokens['input_ids'][b_idx, seq_idx].reshape(1)), dim=0)\n",
    "                #print(total_logits.shape)\n",
    "                #print(total_gts.shape)\n",
    "                #print(seq_idx, label_tokens['input_ids'][b_idx, seq_idx].reshape(1))\n",
    "\n",
    "        loop.set_description(f\"Loss: {iteration_loss.item()}\")\n",
    "    total_preds = torch.argmax(total_logits, dim=1)\n",
    "    test_accuracy = torch_metrics_accuracy(total_preds.reshape(-1),total_gts.reshape(-1), num_classes=30,ignore_idx=0)\n",
    "\n",
    "    for k in range(1, 5):\n",
    "        test_topk_accuracy = my_topk_accuracy(total_logits,total_gts.reshape(-1),topk=k,ignore_idx=0,num_classes=30)\n",
    "        print(f\"Test Top-{k} Accuracy: {test_topk_accuracy}\")\n",
    "\n",
    "    #print(total_preds.shape)\n",
    "    #print(total_gts.shape)\n",
    "    final_pred = total_preds.reshape(-1)\n",
    "    final_gts = total_gts.reshape(-1)\n",
    "    #print(final_pred.shape)\n",
    "    #print(final_gts.shape)\n",
    "\n",
    "    return test_accuracy, total_loss / total_count, 0.0, final_pred, final_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(model,test_loader,device,vocab,data_name):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_gts = torch.zeros((0), dtype=torch.long).to(device)\n",
    "    total_preds = torch.zeros((0), dtype=torch.long).to(device)\n",
    "    total_logits = torch.zeros((0,30),dtype=torch.float).to(device)\n",
    "    loop = tqdm(test_loader,total=len(test_loader),leave=True) \n",
    "    \n",
    "\n",
    "    weight_and_virus = []\n",
    "    for idx, batch in enumerate(loop):\n",
    "        input_tokens, label_tokens, start_id, end_id, name = batch\n",
    "\n",
    "        input_tokens['input_ids'] = input_tokens['input_ids'].to(device).squeeze(1)\n",
    "        input_tokens['attention_mask'] = input_tokens['attention_mask'].to(device).squeeze(1)\n",
    "        input_tokens['token_type_ids'] = input_tokens['token_type_ids'].to(device).squeeze(1)\n",
    "        \n",
    "        label_tokens['input_ids'] = label_tokens['input_ids'].to(device).squeeze(1)\n",
    "        label_tokens['attention_mask'] = label_tokens['attention_mask'].to(device).squeeze(1)\n",
    "        label_tokens['token_type_ids'] = label_tokens['token_type_ids'].to(device).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out, attention = model.forward_test(input_tokens)\n",
    "\n",
    "        \n",
    "        for b_no, (input_token, label_token) in enumerate(zip(input_tokens['input_ids'],label_tokens['input_ids'])):\n",
    "            \n",
    "            ### Integers to Residues\n",
    "            input_str_list = [vocab[int(x)] for x in input_token.tolist()]\n",
    "            #print(input_str_list)\n",
    "            input_str = \"\".join(input_str_list)\n",
    "            #print(input_str)\n",
    "            label_str_list = [vocab[int(x)] for x in label_token.tolist()]\n",
    "            #print(label_str_list)\n",
    "            #label_str_list = [vocab.get(int(x),\"\") for x in label_token.tolist()]\n",
    "            label_str = \"\".join(label_str_list)\n",
    "\n",
    "            input_str = input_str.replace('[CLS]', 'J')\n",
    "            input_str = input_str.replace('[SEP]', 'J')\n",
    "            input_str = input_str.replace('[PAD]', 'J')\n",
    "            input_str = input_str.replace('[UNK]', 'J')\n",
    "            input_str = input_str.replace('[MASK]', 'J')\n",
    "\n",
    "            label_str = label_str.replace('[CLS]', 'J')\n",
    "            label_str = label_str.replace('[SEP]', 'J')\n",
    "            label_str = label_str.replace('[PAD]', 'J')\n",
    "            label_str = label_str.replace('[UNK]', 'J')\n",
    "            label_str = label_str.replace('[MASK]', 'J')\n",
    "\n",
    "\n",
    "            pdb_name = name[b_no]\n",
    "            #print(f'pdb : {pdb_name}')\n",
    "            #print(f\"Input: {input_str[1:]}\")\n",
    "            req_attention = attention[layer_no_to_view]\n",
    "            req_attention = req_attention[b_no, head_no_to_view,:,:].squeeze(0).cpu().numpy()\n",
    "            req_attention = req_attention[1:, 1:]\n",
    "            #print(req_attention.shape)\n",
    "\n",
    "            weight_and_virus.append({\"pdb\": pdb_name ,\"seq\": input_str[1:], \"attention\": req_attention})\n",
    "\n",
    "    \n",
    "    weight_and_virus_np = np.array(weight_and_virus)\n",
    "    np.save(f\"{data_name}_(head{head_no_to_view}).npy\", weight_and_virus_np)\n",
    "    print(weight_and_virus_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary OrderedDict([('[PAD]', 0), ('[UNK]', 1), ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4), ('L', 5), ('A', 6), ('G', 7), ('V', 8), ('E', 9), ('S', 10), ('I', 11), ('K', 12), ('R', 13), ('D', 14), ('T', 15), ('P', 16), ('N', 17), ('Q', 18), ('F', 19), ('Y', 20), ('M', 21), ('H', 22), ('C', 23), ('W', 24), ('X', 25), ('U', 26), ('B', 27), ('Z', 28), ('O', 29)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:06<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(data_path, allow_pickle=True)\n",
    "req_save_name = parameter_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "# Create dataset\n",
    "dataset = PositionPredictionFullDataset(data)\n",
    "\n",
    "# bert_tokenizer\n",
    "bert_vocab = dataset.tokenizer.vocab\n",
    "\n",
    "# Create dataloader\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "model = ClassificationTwoPositionsBert().to(device)\n",
    "\n",
    "# Create optimizer, loss function, scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0,label_smoothing=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=1,verbose=True)\n",
    "inverse_vocab = {v: k for k, v in bert_vocab.items()}\n",
    "\n",
    "save_dict = torch.load(parameter_path)\n",
    "model.load_state_dict(save_dict['model'], strict=False)\n",
    "optimizer.load_state_dict(save_dict['optimizer'])\n",
    "scheduler.load_state_dict(save_dict['scheduler'])\n",
    "epoch = save_dict['epoch']\n",
    "\n",
    "# test first to see if model is properly trained\n",
    "# test_accuracy, test_loss, _, final_pred,final_gts = test_forward_two_position_prediction(model, data_loader, loss_fn, device)\n",
    "\n",
    "# For result of single head\n",
    "#head_no_to_view = 0\n",
    "#inference_test(model,data_loader, device,inverse_vocab, data_name=req_save_name)\n",
    "\n",
    "# For results of all heads\n",
    "for i in range(0, 16):\n",
    "    head_no_to_view = i\n",
    "    inference_test(model,data_loader, device,inverse_vocab, data_name=req_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/andrewkim/Desktop/GPCRBert/proteins_NPXXY_(head2).npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/home/andrewkim/Desktop/GPCRBert/proteins_NPXXY_(head2).npy\u001b[39;49m\u001b[39m'\u001b[39;49m, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m weights[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(weights[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mattention\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/andrewkim/Desktop/GPCRBert/proteins_NPXXY_(head2).npy'"
     ]
    }
   ],
   "source": [
    "weights = np.load('/home/andrewkim/Desktop/GPCRBert/proteins_NPXXY_(head2).npy', allow_pickle=True)\n",
    "weights[0]\n",
    "print(weights[0]['attention'].shape)\n",
    "print(weights.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python.3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
